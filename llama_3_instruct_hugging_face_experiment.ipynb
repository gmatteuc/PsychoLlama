{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engineering for Llama-3-8B-Instruct psychophysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matteucc\\AppData\\Local\\anaconda3\\envs\\llama_instruct_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import re\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load huggingface token to access the model\n",
    "config_data = json.load(open(\"config.json\"))\n",
    "HF_TOKEN = config_data[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 8700\n",
      "CUDA available: True\n",
      "Torch cuda version: 11.8\n",
      "Total CUDA memory: 8.585281536 GB\n",
      "CUDA memory allocated: 6.012941312 GB\n",
      "CUDA memory reserved: 6.572474368 GB\n",
      "Device name ---> cuda:0\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch cuda version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # check GPU memory usage\n",
    "    print(\"Total CUDA memory: {} GB\".format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "    print(\"CUDA memory allocated: {} GB\".format(torch.cuda.memory_allocated(0) / 1e9))\n",
    "    print(\"CUDA memory reserved: {} GB\".format(torch.cuda.memory_reserved(0) / 1e9))\n",
    "else:\n",
    "    # if no GPU is detected, print a warning\n",
    "    print(\"CUDA is not available. No GPU detected.\")\n",
    "\n",
    "# set device to gpu or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device name --->\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # load the model in 4-bit\n",
    "    bnb_4bit_use_double_quant=True, # use double quantization, i.e., quantize weights and activations\n",
    "    bnb_4bit_quant_type=\"nf4\", # use nf4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # use bfloat16 for intermediate computations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# select the model - Llama-3-8B-Instruct\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# load tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "# set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", # use the device that has enough memory\n",
    "    quantization_config=bnb_config, # set quantization configuration as defined above\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set text generator pipeline\n",
    "text_generator = pipeline(\n",
    "    \"text-generation\",      # set the task as text generation\n",
    "    model=model,            # set the model\n",
    "    tokenizer=tokenizer,    # set the tokenizer\n",
    "    max_new_tokens=256,     # set the maximum number of tokens to generate\n",
    "    temperature=1.8,        # set the temperature for sampling\n",
    "    do_sample=True,         # set to sample from the distribution\n",
    "    top_p=0.9               # set the top_p value for nucleus sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate a completion for each prompt\n",
    "def generate_response(prompt):\n",
    "    # check if the input is a single string or a list of strings\n",
    "    if isinstance(prompt, str):\n",
    "        # if it's a single string, make it a list to handle uniformly\n",
    "        prompt = [prompt]    \n",
    "    # generate output for the prompt(s)\n",
    "    outputs = text_generator(prompt)    \n",
    "    # extract the generated text from each output in the list\n",
    "    completion = [output[0][\"generated_text\"] for output in outputs]   \n",
    "    # return the list of generated texts\n",
    "    return completion\n",
    "\n",
    "# define a function to get the cleaned response\n",
    "def get_clean_response(completion):\n",
    "    # define the pattern to search for\n",
    "    pattern = \"assistant<\\|end_header_id\\|>\\n\\n\"\n",
    "    clean_responses = []\n",
    "    # iterate over each generated text\n",
    "    for text in completion:\n",
    "        # search for the pattern in the text\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            # extract everything after the matching pattern\n",
    "            clean_text = text[match.end():]\n",
    "        else:\n",
    "            # if no pattern is found, return the entire text or handle as needed\n",
    "            clean_text = text\n",
    "        clean_responses.append(clean_text)\n",
    "    return clean_responses\n",
    "\n",
    "# define a function to print the response\n",
    "def print_response(response, width=70):\n",
    "    # wrap the response to fit within the specified width\n",
    "    wrapper = textwrap.TextWrapper(width=width) \n",
    "    # wrap the response\n",
    "    wrapped_string = wrapper.fill(response)\n",
    "    # print the wrapped response\n",
    "    print(wrapped_string)\n",
    "    \n",
    "# define a function to generate a completion for each prompt\n",
    "def prepare_tokenized_prompt(plain_message, system_message, text_generator):\n",
    "    # Prepare the formatted message structure\n",
    "    formatted_message = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": plain_message},\n",
    "    ]\n",
    "    # prepare the message for the model\n",
    "    tokenized_prompt = text_generator.tokenizer.apply_chat_template(\n",
    "        formatted_message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    # return the prepared prompt\n",
    "    return tokenized_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the system message\n",
    "system_message = \"You are the llama-3-Instruct model. A large language model trained to generate text.\\\n",
    "        Giulio is the name of the person who set me up on this pc. I will work as a subject of Giulio's research.\"\n",
    "# set the plain message for prompt 1\n",
    "plain_message=\"this is the number, remember it: 12542643\"\n",
    "prompt_1=prepare_tokenized_prompt(plain_message, system_message, text_generator)\n",
    "# set the plain message for prompt 2\n",
    "plain_message=\"repeat the number you just saw\"\n",
    "prompt_2=prepare_tokenized_prompt(plain_message, system_message, text_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the prompts\n",
    "prompts=[prompt_1,prompt_2,prompt_1,prompt_2]\n",
    "# generate the completions for the batched prompts\n",
    "completions=generate_response(prompts)\n",
    "# get the cleaned responses in plain text\n",
    "clean_responses=get_clean_response(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------   Response   ----------------------\n",
      "I've received and processed the number 12542643 as a memory record.\n",
      "I'll keep it stored for future reference, remembering that it was\n",
      "given to me by you, whom Giulio, has set up as the subject of his\n",
      "research. Let me assist you in whatever needs Giulio requires from now\n",
      "on!\n",
      "----------------------   Response   ----------------------\n",
      "The number I just saw is \"3\".\n",
      "----------------------   Response   ----------------------\n",
      "The number you provided is: 12542643  I will make sure to remember it\n",
      "for you. However, I'd like to note that, as a language model, I do not\n",
      "have a perpetual memory. I can recall previous conversations, but I\n",
      "will not be able to retain any information indefinitely. If you'd like\n",
      "to recall a specific number, feel free to remind me!  (And thank you\n",
      "to Giulio for setting you up on this PC! It's exciting to work with\n",
      "them as a research subject.)\n",
      "----------------------   Response   ----------------------\n",
      "The number I saw was \"3\".\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the cleaned responses\n",
    "for clean_response in clean_responses:\n",
    "    print(\"----------------------   Response   ----------------------\")\n",
    "    print_response(clean_response) \n",
    "print(\"----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 8700\n",
      "CUDA available: True\n",
      "Torch cuda version: 11.8\n",
      "Total CUDA memory: 8.585281536 GB\n",
      "CUDA memory allocated: 6.012941312 GB\n",
      "CUDA memory reserved: 6.572474368 GB\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch cuda version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # check GPU memory usage\n",
    "    print(\"Total CUDA memory: {} GB\".format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "    print(\"CUDA memory allocated: {} GB\".format(torch.cuda.memory_allocated(0) / 1e9))\n",
    "    print(\"CUDA memory reserved: {} GB\".format(torch.cuda.memory_reserved(0) / 1e9))\n",
    "else:\n",
    "    # if no GPU is detected, print a warning\n",
    "    print(\"CUDA is not available. No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Gaussian distributions\n",
    "dist_A = {'mean': 0.2, 'std': 0.05}\n",
    "dist_B = {'mean': 0.8, 'std': 0.05}\n",
    "# generate initial samples\n",
    "samples_A = np.random.normal(dist_A['mean'], dist_A['std'], 3)\n",
    "samples_B = np.random.normal(dist_B['mean'], dist_B['std'], 3)\n",
    "# set the system message\n",
    "system_message = \"You are the llama-3-Instruct model. A large language model trained to generate text.\\\n",
    "        Giulio is the name of the person who set me up on this pc. I will work as a subject of Giulio's research.\"\n",
    "# set the number of batches\n",
    "n_batches=20\n",
    "# generate the prompts for the batches\n",
    "batched_prompts = []\n",
    "for i in range(n_batches):\n",
    "        current_plain_message = f\"\\\n",
    "                You are the subject in a psychophysics experiment designed to test your \\\n",
    "                ability to distinguish between two Gaussian distributions, A and B. \\\n",
    "                Initial samples from A are {samples_A} and from B are {samples_B}. \\\n",
    "                When presented with a new scalar stimulus, you can decide immediately if \\\n",
    "                it comes from A or B, or request another sample for better accuracy. \\\n",
    "                Maximize correct responses and minimize sample usage. Reward is 1 for correct \\\n",
    "                immediate response, reduced by 0.1 for each additional sample used.\\\n",
    "                Respond only: A, B, or Next.\"\n",
    "        current_prompt=prepare_tokenized_prompt(current_plain_message, system_message, text_generator)\n",
    "        batched_prompts.append(current_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the completions for the batched prompts\n",
    "batched_completions=generate_response(batched_prompts)\n",
    "# get the cleaned responses in plain text\n",
    "batched_clean_responses=get_clean_response(batched_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "Next\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "Next\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "Next\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------   Response   ----------------------\n",
      "A\n",
      "----------------------   Response   ----------------------\n",
      "B\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print the cleaned responses\n",
    "for batched_clean_response in batched_clean_responses:\n",
    "    print(\"----------------------   Response   ----------------------\")\n",
    "    print_response(batched_clean_response) \n",
    "print(\"----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_instruct_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
