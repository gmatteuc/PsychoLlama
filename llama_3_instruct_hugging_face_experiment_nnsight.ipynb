{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama-3-8B-Instruct Recording and Perturbation\n",
    "\n",
    "## Test of NNsight library capabilities\n",
    "\n",
    "### Overview\n",
    "This notebook testrs how to read and write inner activations of the language model at runtime, by decoding the answer to a question from the LM_head probabilities and interfering with question answering by replacing the input embedding with random numbers.\n",
    "\n",
    "### Issues\n",
    "Need to relaoad the model between one use of \"with model.trace(...):\" and the other. Why? How to avoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matteucc\\AppData\\Local\\anaconda3\\envs\\llama_instruct_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from nnsight import LanguageModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "# load the configuration file\n",
    "config_data = json.load(open(\"config.json\"))\n",
    "HF_TOKEN = config_data[\"HF_TOKEN\"]\n",
    "# set up model quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # load the model in 4-bit\n",
    "    bnb_4bit_use_double_quant=True, # use double quantization, i.e., quantize weights and activations\n",
    "    bnb_4bit_quant_type=\"nf4\", # use nf4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # use bfloat16 for intermediate computations\n",
    ")\n",
    "# set model id\n",
    "model_id='meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "# load tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN)\n",
    "# set pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# load model\n",
    "model = LanguageModel(\n",
    "    model_id,\n",
    "    device_map='cuda:0',\n",
    "    tokenizer=tokenizer,\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN)\n",
    "# print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unperturbed question answer decoding\n",
    "Ask a question to the model, grab output probabilities and extract answer as argmax of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.35s/it]\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\matteucc\\AppData\\Local\\anaconda3\\envs\\llama_instruct_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[14924,   220,  2258,    11,   264,   279,  4851,   315, 22463]],\n",
      "       device='cuda:0')\n",
      "Prediction:  Rome\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"The Coliseum is in the city of\"):\n",
    "    # get the model prediction\n",
    "    token_ids = model.lm_head.output.argmax(dim=-1).save()\n",
    "# print the token ids\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# apply the tokenizer to decode the ids into words after the tracing context.\n",
    "print(\"Prediction:\", model.tokenizer.decode(token_ids[0][-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbed question answer decoding\n",
    "Ask a question to the model, grab output probabilities and extract answer as argmax of that. But this time perturb the embedding content by replacing it with random numbers on runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[ 12167,  73721,  73721,   3853, 106818,  91876,  18745,  18745,  68412]],\n",
      "       device='cuda:0')\n",
      "Prediction: hay\n"
     ]
    }
   ],
   "source": [
    "# need to reinitialize the model to intervene again ---> ISSUE!!\n",
    "model = LanguageModel(\n",
    "    model_id,\n",
    "    device_map='cuda:0',\n",
    "    tokenizer=tokenizer,\n",
    "    quantization_config=bnb_config,\n",
    "    token=HF_TOKEN)\n",
    "# intervene ablating the embed_tokens content\n",
    "with model.trace(\"The Coliseum is in the city of\"):\n",
    "    # set random tensor to replace embed_tokens output\n",
    "    size = (1, 9, 4096) \n",
    "    device = 'cpu'  \n",
    "    rand_tensor = torch.rand(size, device=device, dtype=torch.float16)\n",
    "    # replace the embed_tokens output with the random tensor at runtime\n",
    "    model.model.embed_tokens.output = rand_tensor\n",
    "    # get the model prediction\n",
    "    token_ids = model.lm_head.output.argmax(dim=-1).save()\n",
    "# print the token ids\n",
    "print(\"Token IDs:\", token_ids)\n",
    "# apply the tokenizer to decode the ids into words after the tracing context.\n",
    "print(\"Prediction:\", model.tokenizer.decode(token_ids[0][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 8700\n",
      "CUDA available: True\n",
      "Torch cuda version: 11.8\n",
      "Total CUDA memory: 8.585281536 GB\n",
      "CUDA memory allocated: 5.846328832 GB\n",
      "CUDA memory reserved: 6.247415808 GB\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available - check after running inference\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch cuda version:\", torch.version.cuda)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # check GPU memory usage - check after running inference\n",
    "    print(\"Total CUDA memory: {} GB\".format(torch.cuda.get_device_properties(0).total_memory / 1e9))\n",
    "    print(\"CUDA memory allocated: {} GB\".format(torch.cuda.memory_allocated(0) / 1e9))\n",
    "    print(\"CUDA memory reserved: {} GB\".format(torch.cuda.memory_reserved(0) / 1e9))\n",
    "else:\n",
    "    # if no GPU is detected, print a warning - check after running inference\n",
    "    print(\"CUDA is not available. No GPU detected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_instruct_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
